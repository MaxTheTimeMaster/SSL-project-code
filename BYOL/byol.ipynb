{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "from torchvision.datasets import VOCSegmentation, VOCDetection\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT = \"VOCdevkit\"\n",
    "CHECKPOINT = \"checkpoints/resnet50_byol_imagenet2012.pth.tar\"\n",
    "\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(VOC_CLASSES)}\n",
    "\n",
    "def smart_load_byol_checkpoint_to_resnet50(ckpt_path: str, device='cpu'):\n",
    "    ckpt_path = str(ckpt_path)\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(\n",
    "        ckpt_path,\n",
    "        map_location='cpu',\n",
    "        weights_only=False\n",
    "    )['online_backbone']\n",
    "\n",
    "    if isinstance(ckpt, dict):\n",
    "        for k in ('state_dict', 'model', 'net', 'online_encoder', 'online_network', 'encoder', 'backbone'):\n",
    "            if k in ckpt:\n",
    "                candidate = ckpt[k]\n",
    "                break\n",
    "        else:\n",
    "            candidate = ckpt\n",
    "    else:\n",
    "        candidate = ckpt\n",
    "\n",
    "    if isinstance(candidate, dict) and any(isinstance(v, torch.Tensor) for v in candidate.values()):\n",
    "        sd = copy.deepcopy(candidate)\n",
    "    else:\n",
    "        raise RuntimeError(\"Не удалось извлечь state_dict из checkpoint (структура неожиданна). \"\n",
    "                           \"Посмотрите yaox12/utils/load_and_convert.py в репо для инструкции. \"\n",
    "                           \"Ссылка: https://github.com/yaox12/BYOL-PyTorch (см. utils).\")\n",
    "\n",
    "    def strip_prefix(key):\n",
    "        prefixes = ['module.', 'online_encoder.', 'online_network.', 'net.', 'encoder.']\n",
    "        for p in prefixes:\n",
    "            if key.startswith(p):\n",
    "                return key[len(p):]\n",
    "        return key\n",
    "\n",
    "    new_sd = {}\n",
    "    for k, v in sd.items():\n",
    "        newk = strip_prefix(k)\n",
    "        if newk.startswith('predictor') or newk.startswith('projector') or 'num_batches_tracked' in newk:\n",
    "            continue\n",
    "        new_sd[newk] = v\n",
    "\n",
    "    model = models.resnet50(weights=None)\n",
    "    model.fc = nn.Identity()\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_sd, strict=False)\n",
    "        print(\"Loaded checkpoint into torchvision resnet50 (strict=False).\")\n",
    "    except Exception as e:\n",
    "        print(\"Не удалось загрузить чекпоинт. Ошибка:\", e)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50008c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSegDataset(Dataset):\n",
    "    def __init__(self, root, year='2007', image_set='trainval', transforms=None):\n",
    "        self.ds = VOCSegmentation(root=root, year=year, image_set=image_set, download=False)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.ds[idx]\n",
    "        img = np.array(img)\n",
    "        mask = np.array(mask)\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask'].long()\n",
    "        return img, mask\n",
    "\n",
    "class VOCDatasetForDet(Dataset):\n",
    "    def __init__(self, root, year='2007', image_set='trainval', transforms=None):\n",
    "        self.ds = VOCDetection(root=root, year=year, image_set=image_set, download=False)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        img, ann = self.ds[idx]\n",
    "        objs = ann['annotation'].get('object', [])\n",
    "        if isinstance(objs, dict):\n",
    "            objs = [objs]\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "        for o in objs:\n",
    "            bb = o['bndbox']\n",
    "            xmin, ymin, xmax, ymax = float(bb['xmin']), float(bb['ymin']), float(bb['xmax']), float(bb['ymax'])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(CLASS_TO_IDX[o['name']] + 1)\n",
    "            areas.append((xmax - xmin) * (ymax - ymin))\n",
    "            iscrowd.append(0)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': torch.tensor(areas, dtype=torch.float32) if areas else torch.zeros((0,), dtype=torch.float32),\n",
    "            'iscrowd': torch.tensor(iscrowd, dtype=torch.int64) if iscrowd else torch.zeros((0,), dtype=torch.int64)\n",
    "        }\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "def det_collate_fn(batch):\n",
    "    imgs = [b[0] for b in batch]\n",
    "    tgts = [b[1] for b in batch]\n",
    "    return imgs, tgts\n",
    "\n",
    "def seg_collate_fn(batch):\n",
    "    imgs, masks = zip(*batch)\n",
    "    max_height = max(img.shape[1] for img in imgs)\n",
    "    max_width  = max(img.shape[2] for img in imgs)\n",
    "    padded_imgs, padded_masks = [], []\n",
    "    for img, mask in zip(imgs, masks):\n",
    "        _, h, w = img.shape\n",
    "        pad_h = max_height - h\n",
    "        pad_w = max_width - w\n",
    "        img = F.pad(img, (0, pad_w, 0, pad_h), mode='constant', value=0)\n",
    "        mask = F.pad(mask, (0, pad_w, 0, pad_h), mode='constant', value=255)\n",
    "        padded_imgs.append(img)\n",
    "        padded_masks.append(mask)\n",
    "    imgs = torch.stack(padded_imgs, 0)\n",
    "    masks = torch.stack(padded_masks, 0)\n",
    "    return imgs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_TRAIN_TRANSFORM = A.Compose([\n",
    "    A.RandomHorizontalFlip(),\n",
    "    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "SEG_TEST_TRANSFORM = A.Compose([\n",
    "    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "DET_TRAIN_TRANSFORM = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "DET_TEST_TRANSFORM = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deeplab_with_backbone(backbone: nn.Module, num_classes: int = 21):\n",
    "    model = deeplabv3_resnet50(weights=None, weights_backbone=None, num_classes=num_classes)\n",
    "    src_sd = backbone.state_dict()\n",
    "    tgt_sd = model.backbone.state_dict()\n",
    "    for k, v in src_sd.items():\n",
    "        if k in tgt_sd and tgt_sd[k].shape == v.shape:\n",
    "            tgt_sd[k] = v\n",
    "    model.backbone.load_state_dict(tgt_sd, strict=False)\n",
    "    with torch.no_grad():\n",
    "        ok = torch.allclose(\n",
    "            backbone.conv1.weight,\n",
    "            model.backbone.state_dict()[\"conv1.weight\"].to(backbone.conv1.weight.device),\n",
    "        )\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"Backbone weights were NOT loaded correctly\")\n",
    "    return model\n",
    "\n",
    "def build_fasterrcnn_with_backbone(backbone: nn.Module, num_classes: int = 21):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False, num_classes=num_classes)\n",
    "    try:\n",
    "        model.backbone.body.load_state_dict(backbone.state_dict(), strict=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step):\n",
    "    if step < LR_WARMUP_ITERS:\n",
    "        alpha = step / LR_WARMUP_ITERS\n",
    "        return BASE_LR * (LR_WARMUP_FACTOR * (1 - alpha) + alpha)\n",
    "    elif step < STEP_LR_MILESTONES[0]:\n",
    "        return BASE_LR\n",
    "    elif step < STEP_LR_MILESTONES[1]:\n",
    "        return BASE_LR * STEP_LR_GAMMA\n",
    "    else:\n",
    "        return BASE_LR * STEP_LR_GAMMA * STEP_LR_GAMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation_miou(model, test_loader, device, num_classes=21, ignore_index=255):\n",
    "    model.eval()\n",
    "    metric = JaccardIndex(task='multiclass', num_classes=num_classes, ignore_index=ignore_index).to(device)\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(test_loader, desc=\"Seg Eval\"):\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(imgs)['out']\n",
    "            preds = logits.argmax(1)\n",
    "            metric.update(preds, masks)\n",
    "    miou = metric.compute().item()\n",
    "    return miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation_iters(\n",
    "    model, train_loader, val_loader, device,\n",
    "    max_iters=24000, accumulation_steps=4,  \n",
    "    get_lr=None, val_every=2000,\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=BASE_LR, momentum=MOMENTUM, weight_decay=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    train_iter = iter(train_loader)\n",
    "    train_losses = []\n",
    "    val_mious = []\n",
    "    step = 0\n",
    "    with tqdm(total=max_iters, desc='Segmentation train') as pbar:\n",
    "        while step < max_iters:\n",
    "            optimizer.zero_grad()\n",
    "            loss_accum = 0.0\n",
    "            for inner in range(accumulation_steps):\n",
    "                try:\n",
    "                    imgs, masks = next(train_iter)\n",
    "                except StopIteration:\n",
    "                    train_iter = iter(train_loader)\n",
    "                    imgs, masks = next(train_iter)\n",
    "                imgs = imgs.to(device)\n",
    "                masks = masks.to(device)\n",
    "                lr_now = get_lr(step)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_now\n",
    "                with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "                    output = model(imgs)['out']\n",
    "                    loss = criterion(output, masks) / accumulation_steps\n",
    "                if scaler is not None:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                loss_accum += loss.item()\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            pbar.update(1)\n",
    "            train_losses.append(loss_accum)\n",
    "            step += 1\n",
    "            pbar.set_postfix(loss=loss_accum)\n",
    "            if (step % val_every == 0 or step == max_iters):\n",
    "                miou = evaluate_segmentation_miou(model, val_loader, device, num_classes=21)\n",
    "                model.train()\n",
    "                val_mious.append((step, miou))\n",
    "    return model, (train_losses, val_mious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18947be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc2007_evaluate(model, data_loader, device, iou_threshs=[0.5, 0.75, 1.0], score_thresh=0.05):\n",
    "    model.eval()\n",
    "    gt = {}\n",
    "    preds_by_class = {c: [] for c in range(len(VOC_CLASSES))}\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(data_loader, desc=\"Gather predictions\"):\n",
    "            imgs_device = [im.to(device) for im in imgs]\n",
    "            outputs = model(imgs_device)\n",
    "            for out, t in zip(outputs, targets):\n",
    "                img_id = int(t['image_id'].item())\n",
    "                gt_boxes = t['boxes'].cpu()\n",
    "                gt_labels = t['labels'].cpu()\n",
    "                gt[img_id] = {'boxes': gt_boxes, 'labels': gt_labels, 'detected': [False]*len(gt_labels)}\n",
    "                boxes = out.get('boxes', torch.empty((0,4))).cpu()\n",
    "                labels = out.get('labels', torch.empty((0,), dtype=torch.int64)).cpu()\n",
    "                scores = out.get('scores', torch.empty((0,))).cpu()\n",
    "                for b, lab, s in zip(boxes, labels, scores):\n",
    "                    cls = int(lab.item()) - 1\n",
    "                    if cls < 0 or cls >= len(VOC_CLASSES): continue\n",
    "                    if s < score_thresh: continue\n",
    "                    preds_by_class[cls].append({'img_id': img_id, 'score': float(s.item()), 'box': b.numpy()})\n",
    "    mAPs = []\n",
    "    for iou_thresh in iou_threshs:\n",
    "        ap_per_class = []\n",
    "        for cls in range(len(VOC_CLASSES)):\n",
    "            preds = sorted(preds_by_class[cls], key=lambda x: x['score'], reverse=True)\n",
    "            npos = sum((gt[i]['labels'] == (cls+1)).sum().item() for i in gt)\n",
    "            if npos == 0:\n",
    "                ap_per_class.append(0.0)\n",
    "                continue\n",
    "            tp = np.zeros(len(preds))\n",
    "            fp = np.zeros(len(preds))\n",
    "            for i, p in enumerate(preds):\n",
    "                img_id = p['img_id']\n",
    "                pred_box = p['box']\n",
    "                gt_entry = gt.get(img_id, {'boxes': torch.zeros((0,4)), 'labels': torch.zeros((0,)), 'detected':[]})\n",
    "                gt_boxes = gt_entry['boxes'].numpy() if len(gt_entry['boxes'])>0 else np.zeros((0,4))\n",
    "                gt_labels = gt_entry['labels'].numpy() if len(gt_entry['labels'])>0 else np.zeros((0,))\n",
    "                same_idx = np.where(gt_labels == (cls+1))[0]\n",
    "                if same_idx.size == 0:\n",
    "                    fp[i] = 1\n",
    "                    continue\n",
    "                ious = []\n",
    "                for gi in same_idx:\n",
    "                    ious.append(box_iou(torch.tensor(pred_box).unsqueeze(0), gt_entry['boxes'][gi].unsqueeze(0)).item())\n",
    "                ious = np.array(ious) if ious else np.array([])\n",
    "                if ious.size == 0 or ious.max() < iou_thresh:\n",
    "                    fp[i] = 1\n",
    "                else:\n",
    "                    sorted_idx = np.argmax(ious)\n",
    "                    global_gt_idx = same_idx[sorted_idx]\n",
    "                    if not gt_entry['detected'][global_gt_idx]:\n",
    "                        tp[i] = 1\n",
    "                        gt_entry['detected'][global_gt_idx] = True\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "            tp_c = np.cumsum(tp)\n",
    "            fp_c = np.cumsum(fp)\n",
    "            rec = tp_c / float(npos)\n",
    "            prec = tp_c / np.maximum(tp_c + fp_c, np.finfo(np.float64).eps)\n",
    "            ap = 0.0\n",
    "            for t in np.linspace(0, 1, 11):\n",
    "                p_vals = prec[rec >= t]\n",
    "                p_val = np.max(p_vals) if p_vals.size > 0 else 0.0\n",
    "                ap += p_val / 11.0\n",
    "            ap_per_class.append(ap)\n",
    "        mAP = float(np.mean(ap_per_class))\n",
    "        mAPs.append(mAP)\n",
    "    return mAPs\n",
    "\n",
    "def train_detection_iters(\n",
    "    model, train_loader, val_loader, device,\n",
    "    max_iters=24000, accumulation_steps=4,\n",
    "    get_lr=None, val_every=2000\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad],\n",
    "                                lr=BASE_LR, momentum=MOMENTUM, weight_decay=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    losses_per_step = []\n",
    "    val_maps = []\n",
    "    step = 0\n",
    "    train_iter = iter(train_loader)\n",
    "    optimizer.zero_grad()\n",
    "    with tqdm(total=max_iters, desc='Detection train') as pbar:\n",
    "        while step < max_iters:\n",
    "            optimizer.zero_grad()\n",
    "            loss_accum = 0.0\n",
    "            for _ in range(accumulation_steps):\n",
    "                try:\n",
    "                    imgs, targets = next(train_iter)\n",
    "                except StopIteration:\n",
    "                    train_iter = iter(train_loader)\n",
    "                    imgs, targets = next(train_iter)\n",
    "                imgs = [im.to(device) for im in imgs]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                lr_now = get_lr(step)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_now\n",
    "                with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "                    loss_dict = model(imgs, targets)\n",
    "                    loss = sum(loss_dict.values()) / accumulation_steps\n",
    "                if scaler is not None:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                loss_accum += loss.item()\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses_per_step.append(loss_accum)\n",
    "            pbar.update(1)\n",
    "            step += 1\n",
    "            pbar.set_postfix(loss=loss_accum)\n",
    "            if (step % val_every == 0):\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    maps = voc2007_evaluate(model, val_loader, device)\n",
    "                val_maps.append((step, maps))\n",
    "                model.train()\n",
    "    return model, losses_per_step, val_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_segmentation_sample(\n",
    "    img,\n",
    "    gt_mask,\n",
    "    pred_mask=None,\n",
    "    class_colors=None,\n",
    "    class_names=None,\n",
    "    norm_mean=[0.485,0.456,0.406],\n",
    "    norm_std=[0.229,0.224,0.225]\n",
    "):\n",
    "    import numpy as np\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu()\n",
    "        if img.ndim == 3:\n",
    "            img = img.numpy()\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "    img = img * norm_std + norm_mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    if isinstance(gt_mask, torch.Tensor):\n",
    "        gt_mask = gt_mask.detach().cpu().numpy()\n",
    "    if pred_mask is not None and isinstance(pred_mask, torch.Tensor):\n",
    "        pred_mask = pred_mask.detach().cpu().numpy()\n",
    "    ncols = 3 if pred_mask is not None else 2\n",
    "    plt.figure(figsize=(4*ncols,4))\n",
    "    plt.subplot(1, ncols, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, ncols, 2)\n",
    "    plt.imshow(gt_mask, cmap='tab20', vmin=0, vmax=20)\n",
    "    plt.title('GT')\n",
    "    plt.axis('off')\n",
    "    if pred_mask is not None:\n",
    "        plt.subplot(1, ncols, 3)\n",
    "        plt.imshow(pred_mask, cmap='tab20', vmin=0, vmax=20)\n",
    "        plt.title('Prediction')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pairwise_curves(results, metric_name='val_metric', title='Comparison', ylabel='mAP'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for mode, vals in results.items():\n",
    "        plt.plot(range(len(vals['train_loss'])), vals['train_loss'], label=f'{mode} train loss')\n",
    "    plt.title(f\"{title}\\nTrain loss\")\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for mode, vals in results.items():\n",
    "        xs = vals.get(f'{metric_name}_x', range(len(vals[metric_name])))\n",
    "        plt.plot(xs, vals[metric_name], marker='o', label=f'{mode}')\n",
    "    plt.title(f\"{title}\\nTest {ylabel}\")\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def save_results_json(filename, results):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38677401",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "LR_WARMUP_FACTOR = 0.333\n",
    "LR_WARMUP_ITERS = 1000\n",
    "STEP_LR_MILESTONES = [8000, 10000]\n",
    "STEP_LR_GAMMA = 0.1\n",
    "\n",
    "MAX_ITERS = 12000\n",
    "EFFECTIVE_BATCH = 16\n",
    "REAL_BATCH = 8\n",
    "accumulation_steps = EFFECTIVE_BATCH // REAL_BATCH\n",
    "VAL_EVERY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"segmentation\"   # или \"detection\"\n",
    "\n",
    "MODE = \"supervised\"   # \"supervised\" или \"byol\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "if TASK == \"segmentation\":\n",
    "    seg_train_12 = VOCSegDataset(root=ROOT, year='2012', image_set='train', transforms=SEG_TRAIN_TRANSFORM)\n",
    "    seg_train = ConcatDataset([seg_train_12])\n",
    "    seg_test = VOCSegDataset(root=ROOT, year='2012', image_set='val', transforms=SEG_TEST_TRANSFORM)\n",
    "    seg_train_loader = DataLoader(seg_train, batch_size=REAL_BATCH, shuffle=True, num_workers=4, collate_fn=seg_collate_fn, drop_last=True)\n",
    "    seg_test_loader = DataLoader(seg_test, batch_size=REAL_BATCH, shuffle=False, num_workers=4, collate_fn=seg_collate_fn)\n",
    "\n",
    "    if MODE == \"byol\":\n",
    "        backbone = smart_load_byol_checkpoint_to_resnet50(CHECKPOINT)\n",
    "    elif MODE == \"supervised\":\n",
    "        backbone = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        backbone.fc = nn.Identity()\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    backbone = backbone.to(DEVICE)\n",
    "    seg_model = build_deeplab_with_backbone(backbone, num_classes=21)\n",
    "    seg_model.to(DEVICE)\n",
    "    seg_model, (seg_train_losses, seg_val_mious) = train_segmentation_iters(\n",
    "        seg_model, seg_train_loader, seg_test_loader,\n",
    "        DEVICE,\n",
    "        max_iters=MAX_ITERS,\n",
    "        accumulation_steps=accumulation_steps,\n",
    "        get_lr=get_lr,\n",
    "        val_every=VAL_EVERY,\n",
    "    )\n",
    "    mious_x = [x[0] for x in seg_val_mious]\n",
    "    mious_y = [x[1] for x in seg_val_mious]\n",
    "    results[MODE] = {\n",
    "        'seg_train_losses': seg_train_losses,\n",
    "        'seg_val_mious': mious_y,\n",
    "        'seg_val_mious_x': mious_x,\n",
    "    }\n",
    "    save_results_json(\"segmentation_results.json\", results)\n",
    "\n",
    "    plot_pairwise_curves(\n",
    "        {MODE: {\n",
    "            'train_loss': results[MODE]['seg_train_losses'],\n",
    "            'val_metric': results[MODE]['seg_val_mious'],\n",
    "            'val_metric_x': results[MODE]['seg_val_mious_x'],\n",
    "        }},\n",
    "        metric_name='val_metric',\n",
    "        title=f'Segmentation ({MODE}) loss and mIoU',\n",
    "        ylabel='mIoU'\n",
    "    )\n",
    "\n",
    "    imgs, masks = next(iter(seg_test_loader))\n",
    "    seg_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = seg_model(imgs.to(DEVICE))['out'].cpu()\n",
    "        preds = out.argmax(1)\n",
    "    for i in range(min(3, imgs.shape[0])):\n",
    "        show_segmentation_sample(imgs[i], masks[i], preds[i])\n",
    "\n",
    "elif TASK == \"detection\":\n",
    "    det_train_07 = VOCDatasetForDet(root=ROOT, year='2007', image_set='trainval', transforms=DET_TRAIN_TRANSFORM)\n",
    "    det_train_12 = VOCDatasetForDet(root=ROOT, year='2012', image_set='trainval', transforms=DET_TRAIN_TRANSFORM)\n",
    "    det_train = ConcatDataset([det_train_07, det_train_12])\n",
    "    det_test = VOCDatasetForDet(root=ROOT, year='2007', image_set='test', transforms=DET_TEST_TRANSFORM)\n",
    "    det_train_loader = DataLoader(det_train, batch_size=REAL_BATCH, shuffle=True, num_workers=8, collate_fn=det_collate_fn, drop_last=True)\n",
    "    det_test_loader = DataLoader(det_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=det_collate_fn)\n",
    "    if MODE == \"byol\":\n",
    "        backbone_det = smart_load_byol_checkpoint_to_resnet50(CHECKPOINT)\n",
    "    elif MODE == \"supervised\":\n",
    "        backbone_det = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        backbone_det.fc = nn.Identity()\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    backbone_det = backbone_det.to(DEVICE)\n",
    "    det_model = build_fasterrcnn_with_backbone(backbone_det, num_classes=21)\n",
    "    det_model.to(DEVICE)\n",
    "    det_model, det_train_losses, det_val_maps = train_detection_iters(\n",
    "        det_model, det_train_loader, det_test_loader,\n",
    "        device=DEVICE,\n",
    "        max_iters=MAX_ITERS,\n",
    "        accumulation_steps=accumulation_steps,\n",
    "        get_lr=get_lr,\n",
    "        val_every=VAL_EVERY\n",
    "    )\n",
    "    xm = [x[0] for x in det_val_maps]\n",
    "    ym = [x[1][0] for x in det_val_maps]\n",
    "    results[MODE] = {\n",
    "        'det_train_losses': det_train_losses,\n",
    "        'det_val_map': ym,\n",
    "        'det_val_map_x': xm,\n",
    "    }\n",
    "    save_results_json(\"detection_results.json\", results)\n",
    "    plot_pairwise_curves(\n",
    "        {MODE: {\n",
    "            'train_loss': results[MODE]['det_train_losses'],\n",
    "            'val_metric': results[MODE]['det_val_map'],\n",
    "            'val_metric_x': results[MODE]['det_val_map_x'],\n",
    "        }},\n",
    "        metric_name='val_metric',\n",
    "        title=f'Detection ({MODE}) loss and mAP@0.5',\n",
    "        ylabel='mAP@0.5'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f236a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
