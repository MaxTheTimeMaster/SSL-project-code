{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23902,"sourceType":"datasetVersion","datasetId":18276},{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215},{"sourceId":14166113,"sourceType":"datasetVersion","datasetId":9029693}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os, random, numpy as np\n# import torch\n# from torch.utils.data import DataLoader\n# import torchvision.transforms.functional as TF\n# from torchvision.datasets import VOCDetection, VOCSegmentation\n\n# VOC07_TRAIN_ROOT = \"/kaggle/input/pascal-voc-2007/VOCtrainval_06-Nov-2007\"\n# VOC07_TEST_ROOT  = \"/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007\"\n\n# VOC_CLASSES = [\n#     \"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\n#     \"bus\",\"car\",\"cat\",\"chair\",\"cow\",\n#     \"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\n#     \"pottedplant\",\"sheep\",\"sofa\",\"train\",\"tvmonitor\"\n# ]\n\n# CLASS_TO_IDX = {cls: i + 1 for i, cls in enumerate(VOC_CLASSES)}  \n\n\n# def parse_voc_annotation(target):\n#     objects = target[\"annotation\"].get(\"object\", [])\n#     if isinstance(objects, dict):\n#         objects = [objects]\n\n#     boxes, labels = [], []\n\n#     for obj in objects:\n#         name = obj[\"name\"]\n#         if name not in CLASS_TO_IDX:\n#             continue\n\n#         bbox = obj[\"bndbox\"]\n#         xmin = float(bbox[\"xmin\"]) - 1\n#         ymin = float(bbox[\"ymin\"]) - 1\n#         xmax = float(bbox[\"xmax\"]) - 1\n#         ymax = float(bbox[\"ymax\"]) - 1\n\n#         boxes.append([xmin, ymin, xmax, ymax])\n#         labels.append(CLASS_TO_IDX[name])\n\n#     if len(boxes) == 0:\n#         boxes = torch.zeros((0, 4), dtype=torch.float32)\n#         labels = torch.zeros((0,), dtype=torch.int64)\n#     else:\n#         boxes = torch.tensor(boxes, dtype=torch.float32)\n#         labels = torch.tensor(labels, dtype=torch.int64)\n\n#     return {\"boxes\": boxes, \"labels\": labels}\n\n\n# MEAN = [0.485, 0.456, 0.406]\n# STD  = [0.229, 0.224, 0.225]\n\n\n# def transform_train_detection(img, target):\n#     img = TF.to_tensor(img)\n#     img = TF.normalize(img, MEAN, STD)\n#     target = parse_voc_annotation(target)\n\n#     if random.random() < 0.5:\n#         img = TF.hflip(img)\n#         _, H, W = img.shape\n#         if target[\"boxes\"].shape[0] > 0:\n#             xmin, ymin, xmax, ymax = target[\"boxes\"].unbind(1)\n#             target[\"boxes\"] = torch.stack(\n#                 [W - xmax - 1, ymin, W - xmin - 1, ymax], dim=1\n#             )\n#     return img, target\n\n\n# def transform_val_detection(img, target):\n#     img = TF.to_tensor(img)\n#     img = TF.normalize(img, MEAN, STD)\n#     target = parse_voc_annotation(target)\n#     return img, target\n\n\n# def transform_train_segmentation(img, mask):\n#     if random.random() < 0.5:\n#         img = TF.hflip(img)\n#         mask = TF.hflip(mask)\n\n#     img = TF.to_tensor(img)\n#     img = TF.normalize(img, MEAN, STD)\n#     mask = torch.as_tensor(np.array(mask), dtype=torch.int64)\n#     return img, mask\n\n\n# def transform_val_segmentation(img, mask):\n#     img = TF.to_tensor(img)\n#     img = TF.normalize(img, MEAN, STD)\n#     mask = torch.as_tensor(np.array(mask), dtype=torch.int64)\n#     return img, mask\n\n\n# train_dataset_det = VOCDetection(\n#     root=VOC07_TRAIN_ROOT,\n#     year=\"2007\",\n#     image_set=\"trainval\",\n#     download=False,\n#     transforms=transform_train_detection\n# )\n\n# test_dataset_det = VOCDetection(\n#     root=VOC07_TEST_ROOT,\n#     year=\"2007\",\n#     image_set=\"test\",\n#     download=False,\n#     transforms=transform_val_detection\n# )\n\n\n# train_dataset_seg = VOCSegmentation(\n#     root=VOC07_TRAIN_ROOT,\n#     year=\"2007\",\n#     image_set=\"trainval\",\n#     download=False,\n#     transforms=transform_train_segmentation\n# )\n\n# val_dataset_seg = VOCSegmentation(\n#     root=VOC07_TEST_ROOT,\n#     year=\"2007\",\n#     image_set=\"test\",\n#     download=False,\n#     transforms=transform_val_segmentation\n# )\n\n\n# def collate_fn_det(batch):\n#     images = [b[0] for b in batch]\n#     targets = [b[1] for b in batch]\n#     return images, targets\n\n\n# def collate_fn_seg(batch):\n#     maxH = max(img.shape[1] for img, _ in batch)\n#     maxW = max(img.shape[2] for img, _ in batch)\n\n#     images, masks = [], []\n\n#     for img, mask in batch:\n#         C, H, W = img.shape\n#         padded_img = torch.zeros((C, maxH, maxW))\n#         padded_img[:, :H, :W] = img\n\n#         padded_mask = torch.full((maxH, maxW), 255, dtype=torch.long)\n#         padded_mask[:H, :W] = mask\n\n#         images.append(padded_img)\n#         masks.append(padded_mask)\n\n#     return torch.stack(images), torch.stack(masks)\n\n\n# train_loader_det = DataLoader(\n#     train_dataset_det,\n#     batch_size=4,\n#     shuffle=True,\n#     num_workers=2,\n#     collate_fn=collate_fn_det\n# )\n\n# test_loader_det = DataLoader(\n#     test_dataset_det,\n#     batch_size=1,\n#     shuffle=False,\n#     num_workers=2,\n#     collate_fn=collate_fn_det\n# )\n\n# train_loader_seg = DataLoader(\n#     train_dataset_seg,\n#     batch_size=4,\n#     shuffle=True,\n#     num_workers=2,\n#     collate_fn=collate_fn_seg\n# )\n\n# val_loader_seg = DataLoader(\n#     val_dataset_seg,\n#     batch_size=4,\n#     shuffle=False,\n#     num_workers=2,\n#     collate_fn=collate_fn_seg\n# )\n\n\n# print(\"Detection:\", len(train_dataset_det), len(test_dataset_det))\n# print(\"Segmentation:\", len(train_dataset_seg), len(val_dataset_seg))\n\n# imgs, targets = next(iter(train_loader_det))\n# print(imgs[0].shape, targets[0].keys())\n\n# imgs, masks = next(iter(train_loader_seg))\n# print(imgs.shape, masks.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T13:28:03.774429Z","iopub.execute_input":"2025-12-17T13:28:03.775073Z","iopub.status.idle":"2025-12-17T13:28:04.501156Z","shell.execute_reply.started":"2025-12-17T13:28:03.775036Z","shell.execute_reply":"2025-12-17T13:28:04.500168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torchvision\n# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n# from torchvision.models.detection import FasterRCNN\n# from torchvision.models.segmentation import deeplabv3_resnet50\n\n# moco_checkpoint = torch.load(\"/kaggle/input/pretrained-weights-mocov2/moco_v2_800ep_pretrain.pth.tar\", map_location=\"cpu\")\n# state_dict = moco_checkpoint[\"state_dict\"]\n# new_state_dict = {}\n# for k, v in state_dict.items():\n#     if k.startswith(\"module.encoder_q.\"):\n#         new_key = k[len(\"module.encoder_q.\"):]\n#         new_state_dict[new_key] = v\n#     elif k.startswith(\"encoder_q.\"):\n#         new_key = k[len(\"encoder_q.\"):]\n#         new_state_dict[new_key] = v\n\n# backbone_supervised = resnet_fpn_backbone(\"resnet50\", pretrained=True, trainable_layers=5)\n# model_det_baseline = FasterRCNN(backbone_supervised, num_classes=21)\n\n# backbone_moco = resnet_fpn_backbone(\"resnet50\", pretrained=False, trainable_layers=5)\n# backbone_moco.body.load_state_dict(new_state_dict, strict=False)\n# model_det_moco = FasterRCNN(backbone_moco, num_classes=21)\n\n# model_seg_baseline = deeplabv3_resnet50(pretrained=False, pretrained_backbone=True, num_classes=21, aux_loss=True)\n\n# model_seg_moco = deeplabv3_resnet50(pretrained=False, pretrained_backbone=False, num_classes=21, aux_loss=True)\n# model_seg_moco.backbone.body.load_state_dict(new_state_dict, strict=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T13:28:25.683769Z","iopub.execute_input":"2025-12-17T13:28:25.684891Z","iopub.status.idle":"2025-12-17T13:28:29.221016Z","shell.execute_reply.started":"2025-12-17T13:28:25.684847Z","shell.execute_reply":"2025-12-17T13:28:29.220253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch.nn.functional as F\n\n# def evaluate_detection(model, data_loader):\n#     model.eval()\n#     preds_by_class = {cls: [] for cls in range(1, 21)}  \n#     gt_by_class = {cls: {} for cls in range(1, 21)}     \n#     npos = {cls: 0 for cls in range(1, 21)}             \n\n#     with torch.no_grad():\n#         for img_idx, (images, targets) in enumerate(data_loader):\n#             images = [img.to(device) for img in images]\n#             outputs = model(images)\n#             for i, output in enumerate(outputs):\n#                 idx = img_idx * data_loader.batch_size + i \n#                 for t in targets[i][\"labels\"].tolist():\n#                     if t == 0: \n#                         continue\n#                     gt_boxes = targets[i][\"boxes\"].tolist()\n#                     gt_by_class[t].setdefault(idx, []).append(gt_boxes[targets[i][\"labels\"].tolist().index(t)])\n#                     npos[t] += 1\n#                 pred_boxes = output[\"boxes\"].cpu().numpy()\n#                 pred_scores = output[\"scores\"].cpu().numpy()\n#                 pred_labels = output[\"labels\"].cpu().numpy()\n#                 for pb, ps, pl in zip(pred_boxes, pred_scores, pred_labels):\n#                     if pl == 0:\n#                         continue\n#                     preds_by_class[pl].append((ps, idx, pb))\n#     APs = {}\n#     for cls in range(1, 21):\n#         if npos[cls] == 0:\n#             continue\n#         preds_cls = sorted(preds_by_class[cls], key=lambda x: x[0], reverse=True)\n#         tp = np.zeros(len(preds_cls), dtype=np.float32)\n#         fp = np.zeros(len(preds_cls), dtype=np.float32)\n#         matched = {img_id: np.zeros(len(gt_by_class[cls].get(img_id, [])), dtype=bool) for img_id in gt_by_class[cls]}\n#         for i, (_, img_id, pred_box) in enumerate(preds_cls):\n#             if img_id in gt_by_class[cls]:\n#                 best_iou = 0.0\n#                 best_gt_idx = -1\n#                 for gt_idx, gt_box in enumerate(gt_by_class[cls][img_id]):\n#                     if not matched[img_id][gt_idx]:\n#                         gt_box = np.array(gt_box, dtype=np.float32)\n#                         pred_box = pred_box.astype(np.float32)\n#                         ixmin = max(gt_box[0], pred_box[0])\n#                         iymin = max(gt_box[1], pred_box[1])\n#                         ixmax = min(gt_box[2], pred_box[2])\n#                         iymax = min(gt_box[3], pred_box[3])\n#                         iw = max(ixmax - ixmin + 1.0, 0.0)\n#                         ih = max(iymax - iymin + 1.0, 0.0)\n#                         inter = iw * ih\n#                         gt_area = (gt_box[2]-gt_box[0]+1.0)*(gt_box[3]-gt_box[1]+1.0)\n#                         pred_area = (pred_box[2]-pred_box[0]+1.0)*(pred_box[3]-pred_box[1]+1.0)\n#                         union = gt_area + pred_area - inter\n#                         iou = inter / union if union > 0 else 0.0\n#                         if iou > best_iou:\n#                             best_iou = iou\n#                             best_gt_idx = gt_idx\n#                 if best_iou >= 0.5:\n#                     tp[i] = 1\n#                     matched[img_id][best_gt_idx] = True\n#                 else:\n#                     fp[i] = 1\n#             else:\n#                 fp[i] = 1\n#         tp_cum = np.cumsum(tp)\n#         fp_cum = np.cumsum(fp)\n#         rec = tp_cum / npos[cls]\n#         prec = tp_cum / (tp_cum + fp_cum + 1e-6)\n#         mrec = np.concatenate(([0.0], rec, [1.0]))\n#         mprec = np.concatenate(([0.0], prec, [0.0]))\n#         for j in range(len(mprec)-2, -1, -1):\n#             mprec[j] = max(mprec[j], mprec[j+1])\n#         idxs = np.where(mrec[1:] != mrec[:-1])[0]\n#         ap = 0.0\n#         for j in idxs:\n#             ap += (mrec[j+1] - mrec[j]) * mprec[j+1]\n#         APs[cls] = ap\n#     mAP = np.mean(list(APs.values())) if APs else 0.0\n#     return mAP, APs\n\n# def evaluate_segmentation(model, data_loader):\n#     model.eval()\n#     num_classes = 21  \n#     total_inter = np.zeros(num_classes, dtype=np.int64)\n#     total_union = np.zeros(num_classes, dtype=np.int64)\n#     with torch.no_grad():\n#         for images, masks in data_loader:\n#             images = images.to(device)\n#             masks = masks.to(device)\n#             outputs = model(images)\n#             preds = outputs['out']  \n#             preds = preds.argmax(dim=1)  \n#             for pred, mask in zip(preds, masks):\n#                 pred = pred.cpu().numpy()\n#                 mask = mask.cpu().numpy()\n#                 valid = mask != 255 \n#                 for cls in range(num_classes):\n#                     curr_pred = (pred == cls) & valid\n#                     curr_gt   = (mask == cls) & valid\n#                     inter = np.logical_and(curr_pred, curr_gt).sum()\n#                     union = np.logical_or(curr_pred, curr_gt).sum()\n#                     total_inter[cls] += inter\n#                     total_union[cls] += union\n#     IoUs = []\n#     for cls in range(1, num_classes): \n#         if total_union[cls] == 0:\n#             IoU = 0.0\n#         else:\n#             IoU = total_inter[cls] / total_union[cls]\n#         IoUs.append(IoU)\n#     mIoU = np.mean(IoUs) if IoUs else 0.0\n#     return mIoU, IoUs\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model_det_baseline = model_det_baseline.to(device)\n# model_det_moco     = model_det_moco.to(device)\n# model_seg_baseline = model_seg_baseline.to(device)\n# model_seg_moco     = model_seg_moco.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T13:28:39.576797Z","iopub.execute_input":"2025-12-17T13:28:39.577758Z","iopub.status.idle":"2025-12-17T13:28:40.108131Z","shell.execute_reply.started":"2025-12-17T13:28:39.577721Z","shell.execute_reply":"2025-12-17T13:28:40.107552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# optimizer_det_baseline = torch.optim.SGD(model_det_baseline.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\n# optimizer_det_moco     = torch.optim.SGD(model_det_moco.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\n# scheduler_det_baseline = torch.optim.lr_scheduler.StepLR(optimizer_det_baseline, step_size=20, gamma=0.1)\n# scheduler_det_moco     = torch.optim.lr_scheduler.StepLR(optimizer_det_moco, step_size=20, gamma=0.1)\n\n# optimizer_seg_baseline = torch.optim.SGD(model_seg_baseline.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n# optimizer_seg_moco     = torch.optim.SGD(model_seg_moco.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n# scheduler_seg_baseline = torch.optim.lr_scheduler.StepLR(optimizer_seg_baseline, step_size=40, gamma=0.1)\n# scheduler_seg_moco     = torch.optim.lr_scheduler.StepLR(optimizer_seg_moco, step_size=40, gamma=0.1)\n\n# criterion_seg = torch.nn.CrossEntropyLoss(ignore_index=255)\n\n# num_epochs_det = 5\n# num_epochs_seg = 5\n\n# mAP_baseline_history = []\n# mAP_moco_history = []\n# mIoU_baseline_history = []\n# mIoU_moco_history = []\n\n# print(\"Starting training...\")\n# for epoch in range(1, num_epochs_det+1):\n#     model_det_baseline.train()\n#     model_det_moco.train()\n#     epoch_loss_baseline = 0.0\n#     epoch_loss_moco = 0.0\n#     for images, targets in tqdm(train_loader_det):\n#         images = [img.to(device) for img in images]\n#         targets_baseline = [{k: v.to(device) for k,v in tgt.items()} for tgt in targets]\n#         targets_moco     = [{k: v.to(device) for k,v in tgt.items()} for tgt in targets]\n#         loss_dict_base = model_det_baseline(images, targets_baseline)\n#         losses_base = sum(loss for loss in loss_dict_base.values())\n#         optimizer_det_baseline.zero_grad()\n#         losses_base.backward()\n#         optimizer_det_baseline.step()\n#         epoch_loss_baseline += losses_base.item()\n#         loss_dict_moco = model_det_moco(images, targets_moco)\n#         losses_moco = sum(loss for loss in loss_dict_moco.values())\n#         optimizer_det_moco.zero_grad()\n#         losses_moco.backward()\n#         optimizer_det_moco.step()\n#         epoch_loss_moco += losses_moco.item()\n#     scheduler_det_baseline.step()\n#     scheduler_det_moco.step()\n#     mAP_base, _ = evaluate_detection(model_det_baseline, test_loader_det)\n#     mAP_moco, _ = evaluate_detection(model_det_moco, test_loader_det)\n#     mAP_baseline_history.append(mAP_base)\n#     mAP_moco_history.append(mAP_moco)\n#     print(f\"[Detection] Epoch {epoch}: Baseline mAP={mAP_base:.4f}, MoCo mAP={mAP_moco:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T13:30:18.973823Z","iopub.execute_input":"2025-12-17T13:30:18.974121Z","iopub.status.idle":"2025-12-17T13:31:59.148541Z","shell.execute_reply.started":"2025-12-17T13:30:18.974092Z","shell.execute_reply":"2025-12-17T13:31:59.147549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# plt.figure(figsize=(6,4))\n# plt.plot(range(1, num_epochs_det+1), mAP_baseline_history, label='Baseline (Supervised)', marker='o')\n# plt.plot(range(1, num_epochs_det+1), mAP_moco_history, label='MoCo v2 (Self-Supervised)', marker='s')\n# plt.xlabel('Epoch')\n# plt.ylabel('mAP (VOC 2007 Test)')\n# plt.title('Object Detection mAP vs Epoch')\n# plt.legend()\n# plt.grid(True)\n# plt.tight_layout()\n# plt.savefig('detection_mAP_curve.png')\n# plt.show()\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T13:29:43.555990Z","iopub.status.idle":"2025-12-17T13:29:43.556222Z","shell.execute_reply.started":"2025-12-17T13:29:43.556108Z","shell.execute_reply":"2025-12-17T13:29:43.556123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for epoch in range(1, num_epochs_seg+1):\n#     model_seg_baseline.train()\n#     model_seg_moco.train()\n#     epoch_loss_base = 0.0\n#     epoch_loss_moco = 0.0\n#     for images, masks in tqdm(train_loader_seg):\n#         images = images.to(device)\n#         masks  = masks.to(device)\n#         outputs_base = model_seg_baseline(images)\n#         main_out_base = outputs_base['out']\n#         aux_out_base  = outputs_base['aux'] if 'aux' in outputs_base else None\n#         loss_main_base = criterion_seg(main_out_base, masks)\n#         loss_aux_base = criterion_seg(aux_out_base, masks) if aux_out_base is not None else 0\n#         loss_total_base = loss_main_base + 0.4 * loss_aux_base \n#         optimizer_seg_baseline.zero_grad()\n#         loss_total_base.backward()\n#         optimizer_seg_baseline.step()\n#         epoch_loss_base += loss_total_base.item()\n#         outputs_moco = model_seg_moco(images)\n#         main_out_moco = outputs_moco['out']\n#         aux_out_moco  = outputs_moco['aux'] if 'aux' in outputs_moco else None\n#         loss_main_moco = criterion_seg(main_out_moco, masks)\n#         loss_aux_moco = criterion_seg(aux_out_moco, masks) if aux_out_moco is not None else 0\n#         loss_total_moco = loss_main_moco + 0.4 * loss_aux_moco\n#         optimizer_seg_moco.zero_grad()\n#         loss_total_moco.backward()\n#         optimizer_seg_moco.step()\n#         epoch_loss_moco += loss_total_moco.item()\n#     scheduler_seg_baseline.step()\n#     scheduler_seg_moco.step()\n#     mIoU_base, _ = evaluate_segmentation(model_seg_baseline, val_loader_seg)\n#     mIoU_moco, _ = evaluate_segmentation(model_seg_moco, val_loader_seg)\n#     mIoU_baseline_history.append(mIoU_base)\n#     mIoU_moco_history.append(mIoU_moco)\n#     print(f\"[Segmentation] Epoch {epoch}: Baseline mIoU={mIoU_base:.4f}, MoCo mIoU={mIoU_moco:.4f}\")\n# print(\"Training completed.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.figure(figsize=(6,4))\n# plt.plot(range(1, num_epochs_seg+1), mIoU_baseline_history, label='Baseline (Supervised)', marker='o')\n# plt.plot(range(1, num_epochs_seg+1), mIoU_moco_history, label='MoCo v2 (Self-Supervised)', marker='s')\n# plt.xlabel('Epoch')\n# plt.ylabel('mIoU (VOC 2012 Val)')\n# plt.title('Segmentation mIoU vs Epoch')\n# plt.legend()\n# plt.grid(True)\n# plt.tight_layout()\n# plt.savefig('segmentation_mIoU_curve.png')\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models, datasets\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.ops import FeaturePyramidNetwork\nfrom torchvision.models.detection.backbone_utils import LastLevelMaxPool\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom tqdm import tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nVOC_CLASSES = [\n    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n    'bus', 'car', 'cat', 'chair', 'cow',\n    'diningtable', 'dog', 'horse', 'motorbike', 'person',\n    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n]\nCLASS_TO_IDX = {cls_name: i+1 for i, cls_name in enumerate(VOC_CLASSES)}\n\nMEAN = [0.485, 0.456, 0.406]\nSTD  = [0.229, 0.224, 0.225]\n\nclass VOCDetectionDataset(datasets.VOCDetection):\n    def __init__(self, root, year=\"2007\", image_set=\"train\"):\n        super().__init__(root=root, year=year, image_set=image_set, download=False)\n\n    def __getitem__(self, index):\n        img, target = super().__getitem__(index)\n\n        ann = target[\"annotation\"]\n        width = int(ann[\"size\"][\"width\"])\n        height = int(ann[\"size\"][\"height\"])\n\n        objects = ann.get(\"object\", [])\n        if not isinstance(objects, list):\n            objects = [objects]\n\n        boxes = []\n        labels = []\n\n        for obj in objects:\n            if int(obj.get(\"difficult\", 0)) == 1:\n                continue\n\n            name = obj[\"name\"]\n            if name not in CLASS_TO_IDX:\n                continue\n\n            bbox = obj[\"bndbox\"]\n            xmin = float(bbox[\"xmin\"]) - 1\n            ymin = float(bbox[\"ymin\"]) - 1\n            xmax = float(bbox[\"xmax\"]) - 1\n            ymax = float(bbox[\"ymax\"]) - 1\n\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(CLASS_TO_IDX[name])\n\n        if len(boxes) == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n        else:\n            boxes = torch.tensor(boxes, dtype=torch.float32)\n            labels = torch.tensor(labels, dtype=torch.int64)\n\n        img = transforms.functional.to_tensor(img)\n        img = transforms.functional.normalize(img, MEAN, STD)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([index])\n        }\n\n        return img, target\n\nclass VOCSegmentationDataset(datasets.VOCSegmentation):\n    def __init__(self, root, year=\"2007\", image_set=\"train\", resize=None):\n        super().__init__(root=root, year=year, image_set=image_set, download=False)\n        self.resize = resize\n\n    def __getitem__(self, index):\n        img, mask = super().__getitem__(index)\n\n        if self.resize:\n            img = img.resize(self.resize, Image.BILINEAR)\n            mask = mask.resize(self.resize, Image.NEAREST)\n\n        img = transforms.functional.to_tensor(img)\n        img = transforms.functional.normalize(img, MEAN, STD)\n\n        mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n\n        return img, mask\n\n\nVOC_ROOT_TRAIN = \"/kaggle/input/pascal-voc-2007/VOCtrainval_06-Nov-2007\"\nVOC_ROOT_TEST  = \"/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007\"\n\ntransform_det = transforms.ToTensor()  \nresize_size = (512, 512)  \ntransform_seg_img = transforms.Normalize(mean=MEAN, std=STD)\n\n# Датасеты\ntrain_det_dataset = VOCDetectionDataset(\n    root=VOC_ROOT_TRAIN,\n    year=\"2007\",\n    image_set=\"trainval\"\n)\n\ntest_det_dataset = VOCDetectionDataset(\n    root=VOC_ROOT_TEST,\n    year=\"2007\",\n    image_set=\"test\"\n)\n\n\n\ntrain_seg_dataset = VOCSegmentationDataset(\n    root=VOC_ROOT_TRAIN,\n    year=\"2007\",\n    image_set=\"trainval\",\n    resize=(512, 512)\n)\n\ntest_seg_dataset = VOCSegmentationDataset(\n    root=VOC_ROOT_TEST,\n    year=\"2007\",\n    image_set=\"test\",\n    resize=(512, 512)\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_det_loader = torch.utils.data.DataLoader(train_det_dataset, batch_size=2, shuffle=True, \n                                              collate_fn=lambda batch: tuple(zip(*batch)))\ntest_det_loader  = torch.utils.data.DataLoader(test_det_dataset, batch_size=2, shuffle=False, \n                                              collate_fn=lambda batch: tuple(zip(*batch)))\ntrain_seg_loader = torch.utils.data.DataLoader(train_seg_dataset, batch_size=4, shuffle=True)\ntest_seg_loader  = torch.utils.data.DataLoader(test_seg_dataset, batch_size=4, shuffle=False)\n\ndef load_moco_weights(weights_path):\n    checkpoint = torch.load(weights_path, map_location='cpu')\n    state_dict = checkpoint.get('state_dict', checkpoint)\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith('module.encoder_q.'):\n            new_key = k[len('module.encoder_q.'):]\n        elif k.startswith('encoder_q.'):\n            new_key = k[len('encoder_q.'):]\n        else:\n            new_key = k\n        if new_key.startswith('fc.') or new_key.startswith('module.fc.') or new_key.startswith('encoder_q.fc.'):\n            continue\n        new_state_dict[new_key] = v\n    return new_state_dict\n\nmoco_weights_path = \"/kaggle/input/pretrained-weights-mocov2/moco_v2_800ep_pretrain.pth.tar\"\nmoco_state = load_moco_weights(moco_weights_path)\n\nresnet50_imagenet = models.resnet50(pretrained=True)\nresnet50_moco = models.resnet50(pretrained=False)\nresnet50_moco.load_state_dict(moco_state, strict=False)\n\n\nbackbone_base = resnet_fpn_backbone(\n    backbone_name=\"resnet50\",\n    pretrained=True,         \n    trainable_layers=5\n)\nbackbone_moco = resnet_fpn_backbone(\n    backbone_name=\"resnet50\",\n    pretrained=False,         \n    trainable_layers=5\n)\nnum_classes = 21 \nmodel_det_base = FasterRCNN(backbone_base, num_classes=21)\nmodel_det_moco = FasterRCNN(backbone_moco, num_classes=num_classes)\n\nmodel_seg_base = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=len(VOC_CLASSES)+1)\nmodel_seg_moco = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=len(VOC_CLASSES)+1)\nmodel_seg_base.backbone.load_state_dict(resnet50_imagenet.state_dict(), strict=False)\nmodel_seg_moco.backbone.load_state_dict(resnet50_moco.state_dict(), strict=False)\nif hasattr(model_seg_base, 'aux_classifier'):\n    model_seg_base.aux_classifier = None\n    model_seg_moco.aux_classifier = None\n\n\nmodel_det_base.to(device)\nmodel_det_moco.to(device)\nmodel_seg_base.to(device)\nmodel_seg_moco.to(device)\n\noptimizer_det_base = optim.SGD(model_det_base.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\noptimizer_det_moco = optim.SGD(model_det_moco.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\noptimizer_seg_base = optim.SGD(model_seg_base.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_seg_moco = optim.SGD(model_seg_moco.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler_seg_base = optim.lr_scheduler.StepLR(optimizer_seg_base, step_size=20, gamma=0.1)\nscheduler_seg_moco = optim.lr_scheduler.StepLR(optimizer_seg_moco, step_size=20, gamma=0.1)\n\ncriterion_seg = nn.CrossEntropyLoss(ignore_index=255)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_detection(model, loader):\n    model.eval()\n    gt_boxes_per_class = {cls_idx: [] for cls_idx in range(1, len(VOC_CLASSES)+1)}\n    pred_data_per_class = {cls_idx: [] for cls_idx in range(1, len(VOC_CLASSES)+1)}\n    for images, targets in tqdm(loader):\n        for target in targets:\n            labels = target['labels'].tolist()\n            boxes = target['boxes'].tolist()\n            for lbl, box in zip(labels, boxes):\n                if lbl == 0:\n                    continue\n                gt_boxes_per_class[lbl].append({\"box\": box, \"used\": False})\n    with torch.no_grad():\n        for images, targets in loader:\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n            for output, target in zip(outputs, targets):\n                pred_boxes = output['boxes'].tolist()\n                pred_scores = output['scores'].tolist()\n                pred_labels = output['labels'].tolist()\n                for box, score, lbl in zip(pred_boxes, pred_scores, pred_labels):\n                    if lbl == 0:\n                        continue\n                    pred_data_per_class[lbl].append({\"score\": score, \"box\": box, \"image_id\": target['image_id'].item()})\n    aps = []\n    for cls_idx in range(1, len(VOC_CLASSES)+1):\n        preds = pred_data_per_class[cls_idx]\n        if len(gt_boxes_per_class[cls_idx]) == 0:\n            continue\n        if len(preds) == 0:\n            aps.append(0.0)\n            continue\n        preds.sort(key=lambda x: x['score'], reverse=True)\n        tp = []\n        fp = []\n        gt_for_class = gt_boxes_per_class[cls_idx]\n        total_gt = len(gt_for_class)\n        for pred in preds:\n            best_iou = 0.0\n            best_gt_idx = -1\n            for gt_idx, gt in enumerate(gt_for_class):\n                if gt['used']:\n                    continue\n               \n                box_pred = pred['box']\n                box_gt = gt['box']\n                inter_xmin = max(box_pred[0], box_gt[0])\n                inter_ymin = max(box_pred[1], box_gt[1])\n                inter_xmax = min(box_pred[2], box_gt[2])\n                inter_ymax = min(box_pred[3], box_gt[3])\n                if inter_xmax < inter_xmin or inter_ymax < inter_ymin:\n                    iou = 0.0\n                else:\n                    inter_area = (inter_xmax - inter_xmin) * (inter_ymax - inter_ymin)\n                    pred_area = (box_pred[2] - box_pred[0]) * (box_pred[3] - box_pred[1])\n                    gt_area = (box_gt[2] - box_gt[0]) * (box_gt[3] - box_gt[1])\n                    union_area = pred_area + gt_area - inter_area\n                    iou = inter_area / union_area if union_area > 0 else 0.0\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = gt_idx\n            if best_iou >= 0.5 and best_gt_idx >= 0:\n                if not gt_for_class[best_gt_idx]['used']:\n                    tp.append(1)\n                    fp.append(0)\n                    gt_for_class[best_gt_idx]['used'] = True \n                else:\n                    tp.append(0)\n                    fp.append(1)\n            else:\n                tp.append(0)\n                fp.append(1)\n        tp = np.array(tp)\n        fp = np.array(fp)\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recalls = cum_tp / float(total_gt)\n        precisions = cum_tp / np.maximum(cum_tp + cum_fp, 1e-6)\n        recalls = np.concatenate(([0.0], recalls, [1.0]))\n        precisions = np.concatenate(([1.0], precisions, [0.0]))\n        for i in range(len(precisions)-2, -1, -1):\n            precisions[i] = max(precisions[i], precisions[i+1])\n        indices = np.where(recalls[1:] != recalls[:-1])[0]\n        ap = 0.0\n        for i in indices:\n            ap += (recalls[i+1] - recalls[i]) * precisions[i+1]\n        aps.append(ap)\n    mAP = 100 * np.mean(aps) if aps else 0.0\n    return mAP\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\ndef evaluate_segmentation(model, loader):\n    model.eval()\n    num_classes = len(VOC_CLASSES) + 1 \n    total_intersect = np.zeros(num_classes, dtype=np.int64)\n    total_union = np.zeros(num_classes, dtype=np.int64)\n    with torch.no_grad():\n        for images, masks in tqdm(loader):\n            images = images.to(device)\n            masks = masks.numpy()  #\n            outputs = model(images)['out']  \n            preds = outputs.argmax(dim=1).cpu().numpy()  \n            for pred_mask, true_mask in zip(preds, masks):\n                mask_ignore = (true_mask == 255)\n                for cls_idx in range(num_classes):\n                    pred_i = (pred_mask == cls_idx) & (~mask_ignore)\n                    true_i = (true_mask == cls_idx) & (~mask_ignore)\n                    inter = np.logical_and(pred_i, true_i).sum()\n                    union = np.logical_or(pred_i, true_i).sum()\n                    total_intersect[cls_idx] += inter\n                    total_union[cls_idx] += union\n    ious = []\n    for cls_idx in range(num_classes):\n        if total_union[cls_idx] == 0:\n            continue\n        iou = total_intersect[cls_idx] / float(total_union[cls_idx])\n        ious.append(iou)\n    mIoU = 100 * np.mean(ious) if ious else 0.0\n    return mIoU\n\ndef train_det_epoch(model, optimizer, loader):\n    model.train()\n    total_loss = 0.0\n    for images, targets in tqdm(loader):\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in tgt.items()} for tgt in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        total_loss += losses.item()\n    return total_loss / len(loader)\n\ndef train_seg_epoch(model, optimizer, loader, criterion):\n    model.train()\n    total_loss = 0.0\n    for images, masks in tqdm(loader):\n        images = images.to(device)\n        masks = masks.to(device)\n        outputs = model(images)['out']  \n        loss = criterion(outputs, masks)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T21:37:18.131895Z","iopub.execute_input":"2025-12-17T21:37:18.132530Z","iopub.status.idle":"2025-12-17T21:37:33.159081Z","shell.execute_reply.started":"2025-12-17T21:37:18.132497Z","shell.execute_reply":"2025-12-17T21:37:33.158086Z"}},"outputs":[{"name":"stdout","text":"Начало обучения...\n\n=== Обучение детекции (ResNet50 MoCo v2) ===\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 20/2506 [00:12<24:53,  1.66it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/691895984.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Обучение детекции (ResNet50 MoCo v2) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_det_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_det_moco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_det_moco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_det_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m     \u001b[0mmAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_det_moco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_det_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0mdet_moco_map_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/691895984.py\u001b[0m in \u001b[0;36mtrain_det_epoch\u001b[0;34m(model, optimizer, loader)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;31m# Получаем словарь потерь от модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         detections = self.transform.postprocess(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# the proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mrel_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Distance from center to box's corner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mc_to_c_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mc_to_c_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"\ndet_epochs = 4\nseg_epochs = 25\n\ndet_base_map_history = []\ndet_moco_map_history = []\nseg_base_miou_history = []\nseg_moco_miou_history = []\n\nprint(\"Начало обучения...\")\n\nprint(\"\\n=== Обучение детекции (ResNet50 ImageNet) ===\")\nfor epoch in range(1, det_epochs+1):\n    avg_loss = train_det_epoch(model_det_base, optimizer_det_base, train_det_loader)\n    mAP = evaluate_detection(model_det_base, test_det_loader)\n    det_base_map_history.append(mAP)\n    print(f\"Epoch {epoch}/{det_epochs} - Loss: {avg_loss:.3f}, Val mAP: {mAP:.2f}%\")\n\nprint(\"\\n=== Обучение детекции (ResNet50 MoCo v2) ===\")\nfor epoch in range(1, det_epochs+1):\n    avg_loss = train_det_epoch(model_det_moco, optimizer_det_moco, train_det_loader)\n    mAP = evaluate_detection(model_det_moco, test_det_loader)\n    det_moco_map_history.append(mAP)\n    print(f\"Epoch {epoch}/{det_epochs} - Loss: {avg_loss:.3f}, Val mAP: {mAP:.2f}%\")\n\nprint(\"\\n=== Обучение сегментации (ResNet50 ImageNet) ===\")\nfor epoch in range(1, seg_epochs+1):\n    avg_loss = train_seg_epoch(model_seg_base, optimizer_seg_base, train_seg_loader, criterion_seg)\n    mIoU = evaluate_segmentation(model_seg_base, test_seg_loader)\n    seg_base_miou_history.append(mIoU)\n    print(f\"Epoch {epoch}/{seg_epochs} - Loss: {avg_loss:.3f}, Val mIoU: {mIoU:.2f}%\")\n    scheduler_seg_base.step()\n\nprint(\"\\n=== Обучение сегментации (ResNet50 MoCo v2) ===\")\nfor epoch in range(1, seg_epochs+1):\n    avg_loss = train_seg_epoch(model_seg_moco, optimizer_seg_moco, train_seg_loader, criterion_seg)\n    mIoU = evaluate_segmentation(model_seg_moco, test_seg_loader)\n    seg_moco_miou_history.append(mIoU)\n    print(f\"Epoch {epoch}/{seg_epochs} - Loss: {avg_loss:.3f}, Val mIoU: {mIoU:.2f}%\")\n    scheduler_seg_moco.step()\n\n# Сохранение моделей\ntorch.save(model_det_base.state_dict(), \"fasterrcnn_resnet50_baseline.pth\")\ntorch.save(model_det_moco.state_dict(), \"fasterrcnn_resnet50_moco.pth\")\ntorch.save(model_seg_base.state_dict(), \"deeplabv3_resnet50_baseline.pth\")\ntorch.save(model_seg_moco.state_dict(), \"deeplabv3_resnet50_moco.pth\")\n\nplt.figure()\nepochs_det = range(1, det_epochs+1)\nplt.plot(epochs_det, det_base_map_history, 'o-', label=\"Det mAP (ImageNet)\")\nplt.plot(epochs_det, det_moco_map_history, 'o-', label=\"Det mAP (MoCo v2)\")\nplt.title(\"Detection mAP per Epoch (Pascal VOC 2007)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"mAP (%)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"detection_mAP.png\")\nplt.show()\n\nplt.figure()\nepochs_seg = range(1, seg_epochs+1)\nplt.plot(epochs_seg, seg_base_miou_history, 'o-', label=\"Seg mIoU (ImageNet)\")\nplt.plot(epochs_seg, seg_moco_miou_history, 'o-', label=\"Seg mIoU (MoCo v2)\")\nplt.title(\"Segmentation mIoU per Epoch (Pascal VOC 2007)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"mIoU (%)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"segmentation_mIoU.png\")\nplt.show()\nprint(\"\\nОбучение завершено. Модели и графики сохранены.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}